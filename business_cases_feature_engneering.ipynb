{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3xjkr6BV4ojq83WpcQ8mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelsoumya/python_machine_learning/blob/main/business_cases_feature_engneering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nq6CYRkhMAOl",
        "outputId": "86ab7453-2898-4216-9707-007d6789e91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MODULE 5: ADVANCED TECHNIQUES AND BUSINESS APPLICATIONS\n",
            "============================================================\n",
            "\n",
            "1. TIME SERIES FEATURE ENGINEERING\n",
            "----------------------------------------\n",
            "Time Series Dataset Shape: (1461, 2)\n",
            "\n",
            "First 10 rows:\n",
            "        date       sales\n",
            "0 2020-01-01  102.483571\n",
            "1 2020-01-02  103.424098\n",
            "2 2020-01-03  108.525556\n",
            "3 2020-01-04  110.403151\n",
            "4 2020-01-05   97.484355\n",
            "5 2020-01-06   94.984970\n",
            "6 2020-01-07  105.222700\n",
            "7 2020-01-08  105.278161\n",
            "8 2020-01-09  103.207612\n",
            "9 2020-01-10  109.437699\n",
            "\n",
            "Time Series Features Created:\n",
            "- Basic time features: year, month, day, day_of_week, quarter\n",
            "- Binary features: is_weekend, is_month_start, is_month_end\n",
            "- Cyclical features: month_sin/cos, day_of_week_sin/cos\n",
            "- Lag features: sales_lag_1, sales_lag_7, sales_lag_30\n",
            "- Rolling features: rolling_mean_7, rolling_std_7, rolling_mean_30\n",
            "- Difference features: sales_diff_1, sales_diff_7\n",
            "\n",
            "2. TEXT FEATURE ENGINEERING\n",
            "----------------------------------------\n",
            "Text Dataset:\n",
            "                                                text  sentiment\n",
            "0  I love this product! It's amazing and works pe...          1\n",
            "1  This is the worst purchase I've ever made. Ter...          0\n",
            "2      Good product, fast delivery, would recommend.          1\n",
            "3  Not bad, but could be better. Average experience.          0\n",
            "4  Excellent service and high-quality product. Hi...          1\n",
            "5  Disappointed with the product. Poor customer s...          0\n",
            "6  Great value for money. Very satisfied with pur...          1\n",
            "7  Product arrived damaged. Very unhappy with ser...          0\n",
            "8       Amazing experience! Best purchase ever made.          1\n",
            "9    Okay product, nothing special. Expected better.          0\n",
            "\n",
            "Text Features Created:\n",
            "- Basic features: text_length, word_count, avg_word_length\n",
            "- Sentiment features: positive_word_count, negative_word_count, sentiment_score\n",
            "- TF-IDF features: 10 most important words\n",
            "\n",
            "Text Features (first 5 rows):\n",
            "                                                text  sentiment  text_length  \\\n",
            "0  I love this product! It's amazing and works pe...          1           54   \n",
            "1  This is the worst purchase I've ever made. Ter...          0           60   \n",
            "2      Good product, fast delivery, would recommend.          1           45   \n",
            "3  Not bad, but could be better. Average experience.          0           49   \n",
            "4  Excellent service and high-quality product. Hi...          1           63   \n",
            "\n",
            "   word_count  avg_word_length  positive_word_count  negative_word_count  \\\n",
            "0           9         5.111111                    3                    0   \n",
            "1          10         5.100000                    0                    2   \n",
            "2           6         6.666667                    1                    0   \n",
            "3           8         5.250000                    0                    1   \n",
            "4           7         8.142857                    1                    0   \n",
            "\n",
            "   sentiment_score   amazing   average    better  damaged  delivery  \\\n",
            "0                3  0.845521  0.000000  0.000000      0.0  0.000000   \n",
            "1               -2  0.000000  0.000000  0.000000      0.0  0.000000   \n",
            "2                1  0.000000  0.000000  0.000000      0.0  0.881071   \n",
            "3               -1  0.000000  0.639489  0.543624      0.0  0.000000   \n",
            "4                1  0.000000  0.000000  0.000000      0.0  0.000000   \n",
            "\n",
            "   experience   product  purchase   quality   service  \n",
            "0    0.000000  0.533942  0.000000  0.000000  0.000000  \n",
            "1    0.000000  0.000000  0.658454  0.752621  0.000000  \n",
            "2    0.000000  0.472984  0.000000  0.000000  0.000000  \n",
            "3    0.543624  0.000000  0.000000  0.000000  0.000000  \n",
            "4    0.000000  0.429260  0.000000  0.679753  0.594703  \n",
            "\n",
            "3. DOMAIN-SPECIFIC FEATURE ENGINEERING\n",
            "----------------------------------------\n",
            "\n",
            "3.1 FINANCIAL DOMAIN FEATURES\n",
            "------------------------------\n",
            "Financial Features Created:\n",
            "- Ratios: debt_to_income_ratio, income_per_age, credit_utilization, employment_stability\n",
            "- Categories: income_category, credit_risk\n",
            "\n",
            "3.2 E-COMMERCE DOMAIN FEATURES\n",
            "------------------------------\n",
            "E-commerce Features Created:\n",
            "- Business metrics: basket_size, purchase_frequency, customer_lifetime_value, churn_risk\n",
            "- Segmentation: customer_segment\n",
            "\n",
            "4. ADVANCED FEATURE INTERACTIONS\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot set a DataFrame with multiple columns to the single column income_age_interaction",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3099909449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;31m# Custom interaction features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income_age_interaction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'credit_income_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'credit_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'purchase_power_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'credit_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4300\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4302\u001b[0m         elif (\n\u001b[1;32m   4303\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item_frame_value\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4459\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   4460\u001b[0m                 \u001b[0;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4461\u001b[0m                 \u001b[0;34mf\"column {key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set a DataFrame with multiple columns to the single column income_age_interaction"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Module 5: Advanced Techniques and Business Applications\n",
        "Level 7 Postgraduate Course\n",
        "\n",
        "This module covers advanced feature engineering techniques including time series features,\n",
        "text features, domain-specific features, and comprehensive business case studies.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODULE 5: ADVANCED TECHNIQUES AND BUSINESS APPLICATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Time Series Feature Engineering\n",
        "print(\"\\n1. TIME SERIES FEATURE ENGINEERING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create time series data\n",
        "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
        "n_samples = len(dates)\n",
        "\n",
        "# Generate time series data\n",
        "np.random.seed(42)\n",
        "base_value = 100\n",
        "trend = np.linspace(0, 50, n_samples)\n",
        "seasonal = 10 * np.sin(2 * np.pi * np.arange(n_samples) / 365.25)  # Annual seasonality\n",
        "weekly = 5 * np.sin(2 * np.pi * np.arange(n_samples) / 7)  # Weekly seasonality\n",
        "noise = np.random.normal(0, 5, n_samples)\n",
        "\n",
        "sales = base_value + trend + seasonal + weekly + noise\n",
        "\n",
        "# Create DataFrame\n",
        "ts_df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'sales': sales\n",
        "})\n",
        "\n",
        "print(\"Time Series Dataset Shape:\", ts_df.shape)\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(ts_df.head(10))\n",
        "\n",
        "# Extract time-based features\n",
        "ts_df['year'] = ts_df['date'].dt.year\n",
        "ts_df['month'] = ts_df['date'].dt.month\n",
        "ts_df['day'] = ts_df['date'].dt.day\n",
        "ts_df['day_of_week'] = ts_df['date'].dt.dayofweek\n",
        "ts_df['quarter'] = ts_df['date'].dt.quarter\n",
        "ts_df['is_weekend'] = ts_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "ts_df['is_month_start'] = ts_df['date'].dt.is_month_start.astype(int)\n",
        "ts_df['is_month_end'] = ts_df['date'].dt.is_month_end.astype(int)\n",
        "\n",
        "# Cyclical encoding for periodic features\n",
        "ts_df['month_sin'] = np.sin(2 * np.pi * ts_df['month'] / 12)\n",
        "ts_df['month_cos'] = np.cos(2 * np.pi * ts_df['month'] / 12)\n",
        "ts_df['day_of_week_sin'] = np.sin(2 * np.pi * ts_df['day_of_week'] / 7)\n",
        "ts_df['day_of_week_cos'] = np.cos(2 * np.pi * ts_df['day_of_week'] / 7)\n",
        "\n",
        "# Lag features\n",
        "ts_df['sales_lag_1'] = ts_df['sales'].shift(1)\n",
        "ts_df['sales_lag_7'] = ts_df['sales'].shift(7)\n",
        "ts_df['sales_lag_30'] = ts_df['sales'].shift(30)\n",
        "\n",
        "# Rolling statistics\n",
        "ts_df['sales_rolling_mean_7'] = ts_df['sales'].rolling(window=7, min_periods=1).mean()\n",
        "ts_df['sales_rolling_std_7'] = ts_df['sales'].rolling(window=7, min_periods=1).std()\n",
        "ts_df['sales_rolling_mean_30'] = ts_df['sales'].rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "# Difference features\n",
        "ts_df['sales_diff_1'] = ts_df['sales'].diff(1)\n",
        "ts_df['sales_diff_7'] = ts_df['sales'].diff(7)\n",
        "\n",
        "print(\"\\nTime Series Features Created:\")\n",
        "print(\"- Basic time features: year, month, day, day_of_week, quarter\")\n",
        "print(\"- Binary features: is_weekend, is_month_start, is_month_end\")\n",
        "print(\"- Cyclical features: month_sin/cos, day_of_week_sin/cos\")\n",
        "print(\"- Lag features: sales_lag_1, sales_lag_7, sales_lag_30\")\n",
        "print(\"- Rolling features: rolling_mean_7, rolling_std_7, rolling_mean_30\")\n",
        "print(\"- Difference features: sales_diff_1, sales_diff_7\")\n",
        "\n",
        "# 2. Text Feature Engineering\n",
        "print(\"\\n2. TEXT FEATURE ENGINEERING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create sample text data\n",
        "texts = [\n",
        "    \"I love this product! It's amazing and works perfectly.\",\n",
        "    \"This is the worst purchase I've ever made. Terrible quality.\",\n",
        "    \"Good product, fast delivery, would recommend.\",\n",
        "    \"Not bad, but could be better. Average experience.\",\n",
        "    \"Excellent service and high-quality product. Highly recommended!\",\n",
        "    \"Disappointed with the product. Poor customer service.\",\n",
        "    \"Great value for money. Very satisfied with purchase.\",\n",
        "    \"Product arrived damaged. Very unhappy with service.\",\n",
        "    \"Amazing experience! Best purchase ever made.\",\n",
        "    \"Okay product, nothing special. Expected better.\"\n",
        "]\n",
        "\n",
        "# Create sentiment labels (1: positive, 0: negative)\n",
        "sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Create DataFrame\n",
        "text_df = pd.DataFrame({\n",
        "    'text': texts,\n",
        "    'sentiment': sentiments\n",
        "})\n",
        "\n",
        "print(\"Text Dataset:\")\n",
        "print(text_df)\n",
        "\n",
        "# Basic text features\n",
        "text_df['text_length'] = text_df['text'].str.len()\n",
        "text_df['word_count'] = text_df['text'].str.split().str.len()\n",
        "text_df['avg_word_length'] = text_df['text'].str.split().apply(\n",
        "    lambda x: np.mean([len(word) for word in x]) if x else 0\n",
        ")\n",
        "\n",
        "# Sentiment indicators\n",
        "positive_words = ['love', 'amazing', 'perfect', 'good', 'excellent', 'great', 'best', 'satisfied']\n",
        "negative_words = ['worst', 'terrible', 'bad', 'disappointed', 'poor', 'damaged', 'unhappy']\n",
        "\n",
        "text_df['positive_word_count'] = text_df['text'].str.lower().apply(\n",
        "    lambda x: sum(1 for word in positive_words if word in x)\n",
        ")\n",
        "text_df['negative_word_count'] = text_df['text'].str.lower().apply(\n",
        "    lambda x: sum(1 for word in negative_words if word in x)\n",
        ")\n",
        "text_df['sentiment_score'] = text_df['positive_word_count'] - text_df['negative_word_count']\n",
        "\n",
        "# TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=10, stop_words='english')\n",
        "tfidf_features = tfidf.fit_transform(text_df['text'])\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Combine with original features\n",
        "text_df = pd.concat([text_df, tfidf_df], axis=1)\n",
        "\n",
        "print(\"\\nText Features Created:\")\n",
        "print(\"- Basic features: text_length, word_count, avg_word_length\")\n",
        "print(\"- Sentiment features: positive_word_count, negative_word_count, sentiment_score\")\n",
        "print(\"- TF-IDF features: 10 most important words\")\n",
        "\n",
        "print(\"\\nText Features (first 5 rows):\")\n",
        "print(text_df.head())\n",
        "\n",
        "# 3. Domain-Specific Feature Engineering\n",
        "print(\"\\n3. DOMAIN-SPECIFIC FEATURE ENGINEERING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 3.1 Financial Domain Features\n",
        "print(\"\\n3.1 FINANCIAL DOMAIN FEATURES\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create financial data\n",
        "n_samples = 1000\n",
        "np.random.seed(42)\n",
        "\n",
        "financial_data = {\n",
        "    'income': np.random.normal(50000, 20000, n_samples),\n",
        "    'debt': np.random.normal(20000, 10000, n_samples),\n",
        "    'credit_score': np.random.normal(700, 100, n_samples),\n",
        "    'age': np.random.normal(35, 10, n_samples),\n",
        "    'employment_years': np.random.normal(5, 3, n_samples)\n",
        "}\n",
        "\n",
        "financial_df = pd.DataFrame(financial_data)\n",
        "\n",
        "# Financial ratios and features\n",
        "financial_df['debt_to_income_ratio'] = financial_df['debt'] / (financial_df['income'] + 1e-8)\n",
        "financial_df['income_per_age'] = financial_df['income'] / (financial_df['age'] + 1e-8)\n",
        "financial_df['credit_utilization'] = financial_df['debt'] / (financial_df['income'] * 0.3 + 1e-8)\n",
        "financial_df['employment_stability'] = financial_df['employment_years'] / (financial_df['age'] - 18 + 1e-8)\n",
        "\n",
        "# Risk categories\n",
        "financial_df['income_category'] = pd.cut(financial_df['income'],\n",
        "                                        bins=[0, 30000, 60000, 100000, np.inf],\n",
        "                                        labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "financial_df['credit_risk'] = pd.cut(financial_df['credit_score'],\n",
        "                                    bins=[0, 580, 670, 740, 800, 850],\n",
        "                                    labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
        "\n",
        "print(\"Financial Features Created:\")\n",
        "print(\"- Ratios: debt_to_income_ratio, income_per_age, credit_utilization, employment_stability\")\n",
        "print(\"- Categories: income_category, credit_risk\")\n",
        "\n",
        "# 3.2 E-commerce Domain Features\n",
        "print(\"\\n3.2 E-COMMERCE DOMAIN FEATURES\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create e-commerce data\n",
        "ecommerce_data = {\n",
        "    'purchase_amount': np.random.exponential(50, n_samples),\n",
        "    'items_in_cart': np.random.poisson(3, n_samples),\n",
        "    'days_since_last_purchase': np.random.exponential(30, n_samples),\n",
        "    'total_purchases': np.random.poisson(10, n_samples),\n",
        "    'avg_order_value': np.random.normal(75, 25, n_samples),\n",
        "    'customer_age': np.random.normal(35, 10, n_samples)\n",
        "}\n",
        "\n",
        "ecommerce_df = pd.DataFrame(ecommerce_data)\n",
        "\n",
        "# E-commerce specific features\n",
        "ecommerce_df['basket_size'] = ecommerce_df['purchase_amount'] / (ecommerce_df['items_in_cart'] + 1e-8)\n",
        "ecommerce_df['purchase_frequency'] = 365 / (ecommerce_df['days_since_last_purchase'] + 1e-8)\n",
        "ecommerce_df['customer_lifetime_value'] = ecommerce_df['total_purchases'] * ecommerce_df['avg_order_value']\n",
        "ecommerce_df['churn_risk'] = 1 / (ecommerce_df['days_since_last_purchase'] + 1e-8)\n",
        "\n",
        "# Customer segments\n",
        "ecommerce_df['customer_segment'] = pd.cut(ecommerce_df['customer_lifetime_value'],\n",
        "                                         bins=[0, 500, 1000, 2000, np.inf],\n",
        "                                         labels=['Bronze', 'Silver', 'Gold', 'Platinum'])\n",
        "\n",
        "print(\"E-commerce Features Created:\")\n",
        "print(\"- Business metrics: basket_size, purchase_frequency, customer_lifetime_value, churn_risk\")\n",
        "print(\"- Segmentation: customer_segment\")\n",
        "\n",
        "# 4. Advanced Feature Interactions\n",
        "print(\"\\n4. ADVANCED FEATURE INTERACTIONS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Combine datasets for interaction features\n",
        "combined_df = pd.concat([financial_df, ecommerce_df], axis=1)\n",
        "\n",
        "# Polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "poly_features = poly.fit_transform(combined_df[['income', 'age', 'credit_score']])\n",
        "poly_feature_names = poly.get_feature_names_out(['income', 'age', 'credit_score'])\n",
        "\n",
        "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "combined_df = pd.concat([combined_df, poly_df], axis=1)\n",
        "\n",
        "# Custom interaction features\n",
        "combined_df['income_age_interaction'] = combined_df['income'] * combined_df['age']\n",
        "combined_df['credit_income_ratio'] = combined_df['credit_score'] / (combined_df['income'] / 1000 + 1e-8)\n",
        "combined_df['purchase_power_index'] = (combined_df['income'] * combined_df['credit_score']) / (combined_df['age'] + 1e-8)\n",
        "\n",
        "print(\"Advanced Interaction Features Created:\")\n",
        "print(\"- Polynomial features: income*age, income*credit_score, age*credit_score\")\n",
        "print(\"- Custom interactions: income_age_interaction, credit_income_ratio, purchase_power_index\")\n",
        "\n",
        "# 5. Feature Engineering Pipeline\n",
        "print(\"\\n5. FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"Simple feature engineering pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_names = []\n",
        "\n",
        "    def create_basic_features(self, df):\n",
        "        \"\"\"Create basic numerical features\"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Basic transformations\n",
        "        for col in df.select_dtypes(include=[np.number]).columns:\n",
        "            if col not in ['target', 'sentiment']:  # Skip target variables\n",
        "                df_processed[f'{col}_squared'] = df[col] ** 2\n",
        "                df_processed[f'{col}_log'] = np.log1p(np.abs(df[col]))\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_ratio_features(self, df, feature_pairs):\n",
        "        \"\"\"Create ratio features between pairs of features\"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        for feature1, feature2 in feature_pairs:\n",
        "            if feature1 in df.columns and feature2 in df.columns:\n",
        "                df_processed[f'{feature1}_{feature2}_ratio'] = df[feature1] / (df[feature2] + 1e-8)\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_statistical_features(self, df, window_sizes=[7, 30]):\n",
        "        \"\"\"Create rolling statistical features\"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        for col in df.select_dtypes(include=[np.number]).columns:\n",
        "            if col not in ['target', 'sentiment']:\n",
        "                for window in window_sizes:\n",
        "                    df_processed[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "                    df_processed[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def fit_transform(self, df, target_col=None):\n",
        "        \"\"\"Apply all feature engineering steps\"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Apply feature engineering steps\n",
        "        df_processed = self.create_basic_features(df_processed)\n",
        "        df_processed = self.create_ratio_features(df_processed, [('income', 'age'), ('debt', 'income')])\n",
        "        df_processed = self.create_statistical_features(df_processed)\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = [col for col in df_processed.columns if col not in ['target', 'sentiment']]\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "# Apply pipeline\n",
        "pipeline = FeatureEngineeringPipeline()\n",
        "combined_df_engineered = pipeline.fit_transform(combined_df)\n",
        "\n",
        "print(f\"Original features: {len(combined_df.columns)}\")\n",
        "print(f\"Engineered features: {len(combined_df_engineered.columns)}\")\n",
        "print(f\"New features created: {len(combined_df_engineered.columns) - len(combined_df.columns)}\")\n",
        "\n",
        "# 6. Model Performance with Advanced Features\n",
        "print(\"\\n6. MODEL PERFORMANCE WITH ADVANCED FEATURES\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create target variable for demonstration\n",
        "combined_df_engineered['target'] = (\n",
        "    combined_df_engineered['income'] * 0.3 +\n",
        "    combined_df_engineered['credit_score'] * 0.2 +\n",
        "    combined_df_engineered['customer_lifetime_value'] * 0.1 +\n",
        "    np.random.normal(0, 10, len(combined_df_engineered))\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X = combined_df_engineered.drop(['target'], axis=1)\n",
        "y = combined_df_engineered['target']\n",
        "\n",
        "# Remove non-numeric columns for simplicity\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare different feature sets\n",
        "feature_sets = {\n",
        "    'Original': ['income', 'debt', 'credit_score', 'age', 'employment_years'],\n",
        "    'Basic_Engineered': ['income', 'debt', 'credit_score', 'age', 'employment_years',\n",
        "                        'income_squared', 'credit_score_squared', 'debt_to_income_ratio'],\n",
        "    'Advanced_Engineered': X_train.columns.tolist()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, features in feature_sets.items():\n",
        "    if name == 'Advanced_Engineered':\n",
        "        X_train_subset = X_train\n",
        "        X_test_subset = X_test\n",
        "    else:\n",
        "        X_train_subset = X_train[features]\n",
        "        X_test_subset = X_test[features]\n",
        "\n",
        "    # Handle missing values\n",
        "    X_train_subset = X_train_subset.fillna(X_train_subset.mean())\n",
        "    X_test_subset = X_test_subset.fillna(X_test_subset.mean())\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_subset, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_subset)\n",
        "\n",
        "    # Evaluate\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_subset, y_train, cv=5, scoring='r2')\n",
        "\n",
        "    results[name] = {\n",
        "        'n_features': len(features) if name != 'Advanced_Engineered' else X_train_subset.shape[1],\n",
        "        'MSE': mse,\n",
        "        'R²': r2,\n",
        "        'CV_R²_Mean': cv_scores.mean(),\n",
        "        'CV_R²_Std': cv_scores.std()\n",
        "    }\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(f\"{'Feature Set':<20} {'Features':<10} {'MSE':<10} {'R²':<10} {'CV_R²':<15}\")\n",
        "print(\"-\" * 70)\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name:<20} {metrics['n_features']:<10} {metrics['MSE']:<10.2f} \"\n",
        "          f\"{metrics['R²']:<10.4f} {metrics['CV_R²_Mean']:<10.4f} (+/- {metrics['CV_R²_Std']*2:.4f})\")\n",
        "\n",
        "# 7. Business Case Study: Customer Churn Prediction\n",
        "print(\"\\n7. BUSINESS CASE STUDY: CUSTOMER CHURN PREDICTION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create customer churn dataset\n",
        "np.random.seed(42)\n",
        "n_customers = 1000\n",
        "\n",
        "churn_data = {\n",
        "    'customer_id': range(1, n_customers + 1),\n",
        "    'tenure_months': np.random.exponential(24, n_customers),\n",
        "    'monthly_charges': np.random.normal(65, 20, n_customers),\n",
        "    'total_charges': np.random.normal(1500, 800, n_customers),\n",
        "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_customers),\n",
        "    'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_customers),\n",
        "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_customers),\n",
        "    'online_security': np.random.choice(['Yes', 'No', 'No internet service'], n_customers),\n",
        "    'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], n_customers)\n",
        "}\n",
        "\n",
        "churn_df = pd.DataFrame(churn_data)\n",
        "\n",
        "# Create churn target based on business logic\n",
        "churn_probability = (\n",
        "    (churn_df['tenure_months'] < 12) * 0.3 +\n",
        "    (churn_df['monthly_charges'] > 80) * 0.2 +\n",
        "    (churn_df['contract_type'] == 'Month-to-month') * 0.4 +\n",
        "    (churn_df['payment_method'] == 'Electronic check') * 0.1 +\n",
        "    (churn_df['online_security'] == 'No') * 0.1 +\n",
        "    np.random.normal(0, 0.1, n_customers)\n",
        ")\n",
        "\n",
        "churn_df['churn'] = (churn_probability > 0.5).astype(int)\n",
        "\n",
        "# Feature engineering for churn prediction\n",
        "churn_df['avg_monthly_charge'] = churn_df['total_charges'] / (churn_df['tenure_months'] + 1e-8)\n",
        "churn_df['tenure_years'] = churn_df['tenure_months'] / 12\n",
        "churn_df['contract_commitment'] = churn_df['contract_type'].map({\n",
        "    'Month-to-month': 0, 'One year': 1, 'Two year': 2\n",
        "})\n",
        "churn_df['has_internet'] = (churn_df['internet_service'] != 'No').astype(int)\n",
        "churn_df['has_security'] = (churn_df['online_security'] == 'Yes').astype(int)\n",
        "churn_df['has_support'] = (churn_df['tech_support'] == 'Yes').astype(int)\n",
        "\n",
        "# Risk score\n",
        "churn_df['churn_risk_score'] = (\n",
        "    (churn_df['tenure_months'] < 12) * 30 +\n",
        "    (churn_df['monthly_charges'] > 80) * 20 +\n",
        "    (churn_df['contract_type'] == 'Month-to-month') * 40 +\n",
        "    (churn_df['payment_method'] == 'Electronic check') * 10 +\n",
        "    (churn_df['online_security'] == 'No') * 10\n",
        ")\n",
        "\n",
        "print(\"Customer Churn Features Created:\")\n",
        "print(\"- Business metrics: avg_monthly_charge, tenure_years, contract_commitment\")\n",
        "print(\"- Service features: has_internet, has_security, has_support\")\n",
        "print(\"- Risk assessment: churn_risk_score\")\n",
        "\n",
        "print(f\"\\nChurn Rate: {churn_df['churn'].mean():.2%}\")\n",
        "print(f\"Average Churn Risk Score: {churn_df['churn_risk_score'].mean():.1f}\")\n",
        "\n",
        "# 8. Visualization\n",
        "print(\"\\n8. VISUALIZATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Time series features\n",
        "plt.subplot(3, 4, 1)\n",
        "ts_df[['sales', 'sales_rolling_mean_7', 'sales_rolling_mean_30']].plot()\n",
        "plt.title('Time Series with Rolling Averages')\n",
        "plt.legend()\n",
        "\n",
        "# Text features\n",
        "plt.subplot(3, 4, 2)\n",
        "text_df[['text_length', 'word_count', 'sentiment_score']].boxplot()\n",
        "plt.title('Text Feature Distributions')\n",
        "\n",
        "# Financial features\n",
        "plt.subplot(3, 4, 3)\n",
        "financial_df[['debt_to_income_ratio', 'credit_utilization']].hist(bins=20, alpha=0.7)\n",
        "plt.title('Financial Ratios Distribution')\n",
        "\n",
        "# E-commerce features\n",
        "plt.subplot(3, 4, 4)\n",
        "ecommerce_df['customer_segment'].value_counts().plot(kind='bar')\n",
        "plt.title('Customer Segments')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Model performance comparison\n",
        "plt.subplot(3, 4, 5)\n",
        "methods = list(results.keys())\n",
        "r2_scores = [results[method]['R²'] for method in methods]\n",
        "plt.bar(methods, r2_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
        "plt.title('Model Performance (R² Score)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Churn analysis\n",
        "plt.subplot(3, 4, 6)\n",
        "churn_df.groupby('contract_type')['churn'].mean().plot(kind='bar')\n",
        "plt.title('Churn Rate by Contract Type')\n",
        "plt.ylabel('Churn Rate')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Feature importance\n",
        "plt.subplot(3, 4, 7)\n",
        "best_features = feature_sets['Advanced_Engineered']\n",
        "X_best = X_train[best_features].fillna(X_train[best_features].mean())\n",
        "y_best = y_train\n",
        "\n",
        "model_best = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_best.fit(X_best, y_best)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': best_features,\n",
        "    'importance': model_best.feature_importances_\n",
        "}).sort_values('importance', ascending=True).tail(10)\n",
        "\n",
        "plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
        "plt.title('Top 10 Feature Importance')\n",
        "\n",
        "# Churn risk distribution\n",
        "plt.subplot(3, 4, 8)\n",
        "churn_df['churn_risk_score'].hist(bins=20, alpha=0.7)\n",
        "plt.title('Churn Risk Score Distribution')\n",
        "plt.xlabel('Risk Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. Business Applications Summary\n",
        "print(\"\\n9. BUSINESS APPLICATIONS SUMMARY\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\"\"\n",
        "Comprehensive Business Applications of Advanced Feature Engineering:\n",
        "\n",
        "1. **Financial Services**:\n",
        "   - Credit Risk Assessment: debt_to_income_ratio, credit_utilization, employment_stability\n",
        "   - Fraud Detection: transaction patterns, behavioral biometrics, temporal features\n",
        "   - Investment Portfolio: market indicators, volatility measures, correlation features\n",
        "\n",
        "2. **E-commerce & Retail**:\n",
        "   - Customer Segmentation: RFM analysis, purchase patterns, lifetime value\n",
        "   - Demand Forecasting: seasonal patterns, promotional effects, external factors\n",
        "   - Inventory Optimization: lead time features, demand variability, supplier performance\n",
        "\n",
        "3. **Healthcare**:\n",
        "   - Disease Prediction: biomarker ratios, temporal patterns, genetic interactions\n",
        "   - Treatment Effectiveness: patient history, medication interactions, compliance metrics\n",
        "   - Resource Planning: patient flow patterns, seasonal variations, capacity utilization\n",
        "\n",
        "4. **Manufacturing**:\n",
        "   - Quality Control: sensor fusion, process parameters, environmental factors\n",
        "   - Predictive Maintenance: equipment health indicators, failure patterns, usage metrics\n",
        "   - Supply Chain: lead times, demand variability, supplier performance\n",
        "\n",
        "5. **Marketing & Advertising**:\n",
        "   - Customer Lifetime Value: purchase patterns, engagement metrics, churn indicators\n",
        "   - Campaign Optimization: response patterns, channel effectiveness, timing features\n",
        "   - Personalization: behavioral patterns, preference indicators, contextual features\n",
        "\n",
        "6. **Telecommunications**:\n",
        "   - Customer Churn: usage patterns, service quality, billing issues\n",
        "   - Network Optimization: traffic patterns, capacity utilization, performance metrics\n",
        "   - Fraud Detection: usage anomalies, location patterns, device fingerprinting\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"END OF MODULE 5\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ]
}